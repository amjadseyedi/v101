---
title: Efficient Learning of Restricted Boltzmann Machines Using Covariance Estimates
crossref: acml19
abstract: Learning RBMs using standard algorithms such as CD(k) involves gradient
  descent on the negative log-likelihood. One of the terms in the gradient, which
  involves expectation w.r.t. the model distribution, is intractable and is obtained
  through an MCMC estimate. In this work we show that the Hessian of the log-likelihood
  can be written in terms of covariances of hidden and visible units and hence, all
  elements of the Hessian can also be estimated using the same MCMC samples with small
  extra computational costs. Since inverting the Hessian may be computationally expensive,
  we propose an algorithm that uses inverse of the diagonal approximation of the Hessian,
  instead. This essentially results in parameter-specific adaptive learning rates
  for the gradient descent process and improves the efficiency of learning RBMs compared
  to the standard methods. Specifically we show that using the inverse of diagonal
  approximation of Hessian in the stochastic DC (difference of convex functions) program
  approach results in very efficient learning of RBMs.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: upadhya19a
month: 0
tex_title: Efficient Learning of Restricted Boltzmann Machines Using Covariance Estimates
firstpage: 836
lastpage: 851
page: 836-851
order: 836
cycles: false
bibtex_author: Upadhya, Vidyadhar and Sastry, P S
author:
- given: Vidyadhar
  family: Upadhya
- given: P S
  family: Sastry
date: 2019-10-15
address: 
publisher: PMLR
container-title: Proceedings of The Eleventh Asian Conference on Machine Learning
volume: '101'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 10
  - 15
pdf: http://proceedings.mlr.press/v101/upadhya19a/upadhya19a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
