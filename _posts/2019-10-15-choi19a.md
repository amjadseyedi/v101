---
title: Cell-aware Stacked LSTMs for Modeling Sentences
crossref: acml19
abstract: We propose a method of stacking multiple long short-term memory (LSTM) layers
  for modeling sentences. In contrast to the conventional stacked LSTMs where only
  hidden states are fed as input to the next layer, the suggested architecture accepts
  both hidden and memory cell states of the preceding layer and fuses information
  from the left and the lower context using the soft gating mechanism of LSTMs. Thus
  the architecture modulates the amount of information to be delivered not only in
  horizontal recurrence but also in vertical connections, from which useful features
  extracted from lower layers are effectively conveyed to upper layers. We dub this
  architecture Cell-aware Stacked LSTM (CAS-LSTM) and show from experiments that our
  models bring significant performance gain over the standard LSTMs on benchmark datasets
  for natural language inference, paraphrase detection, sentiment classification,
  and machine translation. We also conduct extensive qualitative analysis to understand
  the internal behavior of the suggested approach.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: choi19a
month: 0
tex_title: Cell-aware Stacked LSTMs for Modeling Sentences
firstpage: 1172
lastpage: 1187
page: 1172-1187
order: 1172
cycles: false
bibtex_author: Choi, Jihun and Kim, Taeuk and Lee, Sang-goo
author:
- given: Jihun
  family: Choi
- given: Taeuk
  family: Kim
- given: Sang-goo
  family: Lee
date: 2019-10-15
address: 
publisher: PMLR
container-title: Proceedings of The Eleventh Asian Conference on Machine Learning
volume: '101'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 10
  - 15
pdf: http://proceedings.mlr.press/v101/choi19a/choi19a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v101/choi19a/choi19a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
