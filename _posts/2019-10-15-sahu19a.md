---
title: Optimal PAC-Bayesian Posteriors for Stochastic Classifiers and their use for
  Choice of SVM Regularization Parameter
crossref: acml19
abstract: 'PAC-Bayesian set up involves a stochastic classifier characterized by a
  posterior distribution on a classifier set, offers a high probability bound on its
  averaged true risk and is robust to the training sample used. For a given posterior,
  this bound captures the trade off between averaged empirical risk and KL-divergence
  based model complexity term. Our goal is to identify an optimal posterior with the
  least PAC-Bayesian bound. We consider a finite classifier set and 5 distance functions:
  KL-divergence, its Pinsker’s and a sixth degree polynomial approximations; linear
  and squared distances. Linear distance based model results in a convex optimization
  problem and we obtain a closed form expression for its optimal posterior. For uniform
  prior, this posterior has full support with weights negative-exponentially proportional
  to number of misclassifications. Squared distance and Pinsker’s approximation bounds
  are possibly quasi-convex and are observed to have single local minimum. We derive
  fixed point equations (FPEs) using partial KKT system with strict positivity constraints.
  This obviates the combinatorial search for subset support of the optimal posterior.
  For uniform prior, exponential search on a full-dimensional simplex can be limited
  to an ordered subset of classifiers with increasing empirical risk values. These
  FPEs converge rapidly to a stationary point, even for a large classifier set when
  a solver fails. We apply these approaches to SVMs generated using a finite set of
  SVM regularization parameter values on 9 UCI datasets. The resulting optimal posteriors
  (on the set of regularization parameters) yield stochastic SVM classifiers with
  tight bounds. KL-divergence based bound is the tightest, but is computationally
  expensive due to its non-convex nature and multiple calls to a root finding algorithm.
  Optimal posteriors for all 5 distance functions have lowest 10% test error values
  on most datasets, with that of linear distance being the easiest to obtain.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: sahu19a
month: 0
tex_title: Optimal PAC-Bayesian Posteriors for Stochastic Classifiers and their use
  for Choice of SVM Regularization Parameter
firstpage: 268
lastpage: 283
page: 268-283
order: 268
cycles: false
bibtex_author: Sahu, Puja and Hemachandra, Nandyala
author:
- given: Puja
  family: Sahu
- given: Nandyala
  family: Hemachandra
date: 2019-10-15
address: 
publisher: PMLR
container-title: Proceedings of The Eleventh Asian Conference on Machine Learning
volume: '101'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 10
  - 15
pdf: http://proceedings.mlr.press/v101/sahu19a/sahu19a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
