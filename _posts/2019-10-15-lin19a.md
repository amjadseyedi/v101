---
title: Unified Policy Optimization for Robust Reinforcement Learning
crossref: acml19
abstract: Recent years have witnessed significant progress in solving challenging
  problems across various domains using deep reinforcement learning (RL). Despite
  the success, the weak robustness has risen as a big obstacle for applying existing
  RL algorithms into real problems. In this paper, we propose unified policy optimization
  (UPO), a sample-efficient shared policy framework that allows a policy to update
  itself by considering different gradients generated by different policy gradient
  (PG) methods. Specifically, we propose two algorithms called UPO-MAB and UPO-ES,
  to leverage these different gradients by adopting the idea of multi-arm bandit (MAB)
  and evolution strategies (ES), with the purpose of finding the gradient direction
  leading to more performance gain with less extra data cost. Extensive experiments
  show that our approach can lead to stronger robustness and better performance than
  baselines.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: lin19a
month: 0
tex_title: Unified Policy Optimization for Robust Reinforcement Learning
firstpage: 395
lastpage: 410
page: 395-410
order: 395
cycles: false
bibtex_author: Lin, Zichuan and Zhao, Li and Bian, Jiang and Qin, Tao and Yang, Guangwen
author:
- given: Zichuan
  family: Lin
- given: Li
  family: Zhao
- given: Jiang
  family: Bian
- given: Tao
  family: Qin
- given: Guangwen
  family: Yang
date: 2019-10-15
address: 
publisher: PMLR
container-title: Proceedings of The Eleventh Asian Conference on Machine Learning
volume: '101'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 10
  - 15
pdf: http://proceedings.mlr.press/v101/lin19a/lin19a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
