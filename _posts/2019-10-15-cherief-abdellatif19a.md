---
title: A Generalization Bound for Online Variational Inference
crossref: acml19
abstract: Bayesian inference provides an attractive online-learning framework to analyze
  sequential data, and offers generalization guarantees which hold even with model
  mismatch and adversaries. Unfortunately, exact Bayesian inference is rarely feasible
  in practice and approximation methods are usually employed, but do such methods
  preserve the generalization properties of Bayesian inference ? In this paper, we
  show that this is indeed the case for some variational inference (VI) algorithms.
  We consider a few existing online, tempered VI algorithms, as well as a new algorithm,
  and derive their generalization bounds. Our theoretical result relies on the convexity
  of the variational objective, but we argue that the result should hold more generally
  and present empirical evidence in support of this. Our work in this paper presents
  theoretical justifications in favor of online algorithms relying on approximate
  Bayesian methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: cherief-abdellatif19a
month: 0
tex_title: A Generalization Bound for Online Variational Inference
firstpage: 662
lastpage: 677
page: 662-677
order: 662
cycles: false
bibtex_author: Ch\'erief-Abdellatif, Badr-Eddine and Alquier, Pierre and Khan, Mohammad
  Emtiyaz
author:
- given: Badr-Eddine
  family: Ch√©rief-Abdellatif
- given: Pierre
  family: Alquier
- given: Mohammad Emtiyaz
  family: Khan
date: 2019-10-15
address: 
publisher: PMLR
container-title: Proceedings of The Eleventh Asian Conference on Machine Learning
volume: '101'
genre: inproceedings
issued:
  date-parts:
  - 2019
  - 10
  - 15
pdf: http://proceedings.mlr.press/v101/cherief-abdellatif19a/cherief-abdellatif19a.pdf
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v101/cherief-abdellatif19a/cherief-abdellatif19a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
